{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data collection\n",
    "In this notebook, we pull stock and company financials data from alpha vantage.\n",
    "First, we import the necessary packages below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import usual packages for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#import usual packages for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#import datetime to get the current date\n",
    "import datetime as dt\n",
    "\n",
    "#import alpha vantage packages\n",
    "from alpha_vantage.timeseries import TimeSeries #for stock data\n",
    "from alpha_vantage.techindicators import TechIndicators #for technical indicators\n",
    "from alpha_vantage.fundamentaldata import FundamentalData #for company overview\n",
    "from alpha_vantage.alphaintelligence import AlphaIntelligence #for news sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use alpha vantage, we need an API key, which can be obtained at this link:\n",
    "<https://www.alphavantage.co/support/#api-key>\n",
    "\n",
    "Note: the free API key allows up to 25 queries per day.\n",
    "\n",
    "After obtaining our personal key, we initialize the four classes from which we will pull data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Arvind's API key\n",
    "my_key='DJ3QKKTFZ5J298QY'\n",
    "\n",
    "ts = TimeSeries(key=my_key, output_format='pandas')\n",
    "fd = FundamentalData(key=my_key, output_format='pandas')\n",
    "ai = AlphaIntelligence(key=my_key, output_format='pandas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we write a few simple functions that will pull the data, apply some pre-processing, and then save to csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to get stock data and save to csv\n",
    "def stock_to_csv(ticker):\n",
    "    df, _ = ts.get_daily(symbol=ticker, outputsize='full')\n",
    "    df.columns = ['open','high','low','close','volume']\n",
    "    df.sort_index(inplace=True)\n",
    "    df = df[df.index.year >= 2014]\n",
    "    df.to_csv(f'../data/{ticker}_stock.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def news_grouper(df):\n",
    "    df = pd.DataFrame({'date': pd.to_datetime(df['time_published']).dt.date,\n",
    "                       'ticker_sentiment': df['ticker_sentiment']})\n",
    "    return df.groupby('date').sum()\n",
    "\n",
    "def news_cleaner(df):\n",
    "    scores = [] #to store daily sentiment scores for aapl\n",
    "    relevances = [] #to store daily relevance scores for aapl\n",
    "    for i in range(len(df)):\n",
    "        temp = pd.DataFrame(df['ticker_sentiment'].iloc[i])\n",
    "        temp = temp[temp['ticker'] == ticker]\n",
    "        wts = temp['relevance_score'].astype(float)\n",
    "        raw_scores = temp['ticker_sentiment_score'].astype(float)\n",
    "        #append the mean relevance score for the day\n",
    "        relevances.append(wts.mean())\n",
    "        #append the weighted average sentiment score for the day\n",
    "        scores.append(np.dot(wts,raw_scores) / wts.sum())\n",
    "    df[f'{ticker}_sentiment_score'] = np.array(scores)\n",
    "    df[f'{ticker}_relevance_score'] = np.array(relevances)\n",
    "    df.drop('ticker_sentiment', axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "def news_to_csv(ticker, time_from, time_to):\n",
    "    queries = []\n",
    "    start_date = pd.to_datetime(time_from).date()\n",
    "    count = 0\n",
    "    while(count <= 10):\n",
    "        df, _ = ai.get_news_sentiment(tickers=ticker, \n",
    "                                    topics='earnings',\n",
    "                                    time_from=time_from, \n",
    "                                    time_to=time_to,\n",
    "                                    limit=1000)\n",
    "        df.reset_index(inplace=True, drop=True)\n",
    "        queries.append(df)\n",
    "        earliest_date = pd.to_datetime(df['time_published'].iloc[-1]).date()\n",
    "        if earliest_date <= start_date:\n",
    "            break\n",
    "        time_to = pd.to_datetime(df['time_published'].iloc[-1]).strftime('%Y%m%dT%H%M')\n",
    "    df = pd.concat(queries)\n",
    "    df.to_csv(f'../data/{ticker}_news_data_raw.csv')\n",
    "    df = news_grouper(df)\n",
    "    df = news_cleaner(df)\n",
    "    df.to_csv(f'../data/{ticker}_news.csv')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_to_csv('TSLA', time_from='20230101T0130', time_to='20240724T0130')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_to_csv('META', time_from='20230101T0130', time_to='20240803T0130')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_to_csv('GOOG', time_from='20230101T0130', time_to='20240815T0130')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_to_csv('AAPL',time_from='20230101T0130', time_to='20240709T0130')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_to_csv('DIS', time_from='20230101T0130', time_to='20240815T0130')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_to_csv('AMZN', time_from='20230101T0130', time_to='20240626T0130')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_to_csv('NFLX', time_from='20230101T0130', time_to='20240625T0130')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def financial_cleaner(df):\n",
    "    df['date'] = pd.to_datetime(df['fiscalDateEnding'])\n",
    "    df.drop(columns='fiscalDateEnding',inplace=True)\n",
    "    df.set_index('date',inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "    df = df[df.index.year >= 2014]\n",
    "    return df\n",
    "\n",
    "def financials_to_csv(ticker):\n",
    "    df_e, _ = fd.get_earnings_quarterly(ticker)\n",
    "    df_b, _ = fd.get_balance_sheet_quarterly(ticker)\n",
    "    df_i, _ = fd.get_income_statement_quarterly(ticker)\n",
    "    df_e.drop(columns=['reportTime'],inplace=True)\n",
    "    df_b.drop(columns=['reportedCurrency'], inplace=True)\n",
    "    df_i.drop(columns=['reportedCurrency'], inplace=True)\n",
    "    #cleaning the data\n",
    "    df_b = financial_cleaner(df_b)\n",
    "    df_i = financial_cleaner(df_i)\n",
    "    df_e = financial_cleaner(df_e)\n",
    "    #concatenating the dataframes\n",
    "    df_financials = pd.concat([df_b,df_i,df_e],axis=1)\n",
    "    df_financials.to_csv(f'../data/{ticker}_financials.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read in the csv's, some care is needed to ensure that the dtypes of the columns are correctly interpreted. For convenience, we wrap this in a simple function that pulls the data for a given ticker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(ticker):\n",
    "    df_stock = pd.read_csv(f'../data/{ticker}_stock.csv', index_col='date',parse_dates=True,dtype=float)\n",
    "    df_news = pd.read_csv(f'../data/{ticker}_news_data.csv', index_col='date',parse_dates=True,dtype=float)\n",
    "    df_financials = pd.read_csv(f'../data/{ticker}_financials.csv', \n",
    "                                index_col='date',\n",
    "                                parse_dates=True,\n",
    "                                dtype={col: 'float' for col in pd.read_csv(f'../data/{ticker}_financials.csv', nrows=1).columns if col!='reportedDate'},\n",
    "                                na_values='None')\n",
    "    return {'stock': df_stock, \n",
    "            'news' : df_news, \n",
    "            'financials' : df_financials}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_may_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
